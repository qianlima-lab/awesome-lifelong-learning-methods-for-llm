# üöÄ [ACM Computing Surveys 2025] Awesome Lifelong Learning Methods for Large Language Models (Updated Regularly; Latest Papers from NIPS2024, EMNLP2024, COLING2025, AAAI2025, and ICLR2025)

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![arXiv](https://img.shields.io/badge/arXiv-lifelong_LLM-b31b1b.svg)](https://arxiv.org/pdf/2406.06391)

## üì¢ Latest News  
- **2025.06** - We are excited to release the first benchmark [LifelongAgentBench](https://caixd-220529.github.io/LifelongAgentBench/) for lifelong learning of LLM Agents. The paper, source code, datasets are all available!
- **2025.02** ‚Äì Our survey **"Towards Lifelong Learning of Large Language Models: A Survey"** is accepted by ACM Computing Surveys! üôå
- **2025.01** ‚Äì The latest research from **NIPS 2024, EMNLP 2024, COLING 2025, AAAI 2025, and ICLR 2025** is now available! Stay updated with the newest advancements in the field.  
- **2024.12** ‚Äì Our survey **"Towards Lifelong Learning of Large Language Models: A Survey"** has been featured on [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/hilVDi-HAac_1DKvv8jv8Q) and [Áü•‰πé](https://zhuanlan.zhihu.com/p/3352669117)! Check out these platforms for an in-depth interpretation.  
- **2024.06** ‚Äì We have officially released our survey paper **"Towards Lifelong Learning of Large Language Models: A Survey"**! This repository is now live alongside the paper‚Äîfeel free to contribute by opening pull requests to add your papers! üöÄ  


## Introduction üìú 
This repository collects awesome surveys, resources, and papers for **Lifelong Learning** with **Large Language Models**.  
We define 12 lifelong learning scenarios as follows. Please refer to [this survey](https://arxiv.org/abs/2406.06391) for a detailed introduction.  
![image](https://github.com/user-attachments/assets/2d448584-2225-4ef2-9dc8-c94025958f83)

Additionally, you can refer to [this repository](https://github.com/qianlima-lab/awesome-lifelong-llm-agent) for **lifelong learning of LLM agents**.

## Survey üìö
- [Lifelong Learning of Large Language Model-based Agents: A Roadmap](https://arxiv.org/abs/2501.07278) (arXiv 2025.01)
- [Towards Lifelong Learning of Large Language Models: A Survey](https://arxiv.org/abs/2406.06391) (arXiv 2024.06)
- [Recent Advances of Foundation Language Models-based Continual Learning: A Survey](https://arxiv.org/abs/2405.18653) (arXiv 2024.05)
- [Continual Learning of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2404.16789) (arXiv 2024.04)
- [Towards Incremental Learning in Large Language Models: A Critical Review](https://arxiv.org/abs/2404.18311) (arXiv 2024.04)
- [Continual Learning for Large Language Models: A Survey](https://arxiv.org/abs/2402.01364) (arXiv 2024.02)
- [Continual Learning with Pre-Trained Models: A Survey](https://arxiv.org/abs/2401.16386) (arXiv 2024.01)
- [A Comprehensive Survey of Continual Learning: Theory, Method and Application](https://ieeexplore.ieee.org/abstract/document/10444954) (TPAMI 2024)
- [How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances](https://aclanthology.org/2023.emnlp-main.516/) (EMNLP 2023)
- [Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need](https://arxiv.org/abs/2303.07338) (arXiv 2023.03)
- [Deep Class-Incremental Learning: A Survey](https://arxiv.org/abs/2302.03648) (arXiv 2023.03)
- [A Comprehensive Survey of Continual Learning: Theory, Method and Application](https://arxiv.org/abs/2302.00487) (arXiv 2023.02)
- [Continual Learning of Natural Language Processing Tasks: A Survey](https://arxiv.org/abs/2211.12701) (arXiv 2022.11)
- [Continual Lifelong Learning in Natural Language Processing: A Survey](https://aclanthology.org/2020.coling-main.574/) (COLING 2020)

## Codebase üñ•Ô∏è
- [Codebase for Incremental Learning with LLMs](https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm)
- [UIC-Liu-Lab ContinualLM](https://github.com/UIC-Liu-Lab/ContinualLM)
- [Mammoth](https://github.com/aimagelab/mammoth)
- [RevisitingCIL](https://github.com/zhoudw-zdw/RevisitingCIL)
- [PyCIL](https://github.com/G-U-N/PyCIL)
- [PyContinual](https://github.com/ZixuanKe/PyContinual)

## üìÑ Related Papers

### Keywords üè∑Ô∏è

![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) **Continual Vertical Domain Pretraining**  
![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) **Continual Language Domain Pretraining**  
![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) **Continual Temporal Domain Pretraining**  
![](https://img.shields.io/badge/Continual_Text_Classification-green) **Continual Text Classification**  
![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) **Continual Named Entity Recognition**  
![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) **Continual Relation Extraction**  
![](https://img.shields.io/badge/Continual_Machine_Translation-violet) **Continual Machine Translation**  
![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) **Continual Instruction-Tuning**  
![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) **Continual Knowledge Editing**  
![](https://img.shields.io/badge/Continual_Alignment-lightgray) **Continual Alignment**  
![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) **Retrieval-Based Lifelong Learning**  
![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) **Tool-Based Lifelong Learning**  
![](https://img.shields.io/badge/Mechanism-gray) **Mechanism**  

### Paper List üìö

#### 2025 (2025.2-2025.5) üìÖ
Coming soon! ‚è≥

#### 2024 (2024.11-2025.1) üìÖ

- **Spurious Forgetting in Continual Learning of Language Models**, ICLR 2025.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)![](https://img.shields.io/badge/Continual_Alignment-lightgray)

- **Knowledge And Capability Transfer Through Large Language Models' Parameters Fusing**, ICLR 2025.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **CollabEdit: Towards Non-destructive Collaborative Knowledge Editing**, ICLR 2025.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code**, ICLR 2025.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment**, ICLR 2025.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Self-Updatable Large Language Models with Parameter Integration**, ICLR 2025.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Synthetic continued pretraining**, ICLR 2025.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Last Iterate Convergence of Incremental Methods as a Model of Forgetting**, ICLR 2025. ![](https://img.shields.io/badge/Mechanism-gray) 

- **Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning**, ICLR 2025.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)![](https://img.shields.io/badge/Mechanism-gray) 

- **Perturbation-Restrained Sequential Model Editing**, ICLR 2025.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **In-Context Editing: Learning Knowledge from Self-Induced Distributions**, ICLR 2025.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance Perspective**, AAAI 2025.![](https://img.shields.io/badge/Continual_Relation_Extraction-blue)

- **CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models**, AAAI 2025.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Continual Learning Using a Kernel-Based Method Over Foundation Models**, AAAI 2025.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **CareBot: A Pioneering Full-Process Open-Source Medical Language Model**, AAAI 2025.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-orange)

- **Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation on Nepali**, COLING 2025.![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange)

- **In-context Continual Learning Assisted by an External Continual Learner**, COLING 2025.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **Continual Learning Using Only Large Language Model Prompting**, COLING 2025.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **Rethinking Long Context Generation from the Continual Learning Perspective**, COLING 2025.![](https://img.shields.io/badge/Mechanism-gray)

- **Dynamic-prototype Contrastive Fine-tuning for Continual Few-shot Relation Extraction with Unseen Relation Detection**, COLING 2025.![](https://img.shields.io/badge/Continual_Relation_Extraction-blue)

- **Discarding the Crutches: Adaptive Parameter-Efficient Expert Meta-Learning for Continual Semantic Parsing**, COLING 2025. (Semantic Parsing)

- **TL-CL: Task And Language Incremental Continual Learning**, EMNLP 2024.![](https://img.shields.io/badge/Continual_Machine_Translation-violet)

- **SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models**, EMNLP 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Mitigating Catastrophic Forgetting in Language Transfer via Model Merging**, Findings of EMNLP 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Gradient Localization Improves Lifelong Pretraining of Language Models**, Findings of EMNLP 2024. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-red)

- **Learn more, but bother less: parameter efficient continual learning**, NIPS 2024.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **Continual Learning with Global Alignment**, NIPS 2024.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **Continual Audio-Visual Sound Separation**, NIPS 2024. (Audio)

- **Should We Really Edit Language Models? On the Evaluation of Edited Language Models**, NIPS 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Continual Learning with Embedding Layer Surgery and Task-wise Beam Search using Whisper**, IEEE Spoken Language Technology Workshop. (Speech)

- **Chained Tuning Leads to Biased Forgetting**, ICML 2024 Workshop.![](https://img.shields.io/badge/Mechanism-gray) 

- **LOIRE: LifelOng learning on Incremental data via pre-trained language model gRowth Efficiently**, OpenReview Preprint. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Learning Beyond the Surface: How Far Can Continual Pre-Training with LoRA Enhance LLMs' Domain-Specific Insight Learning?**, Preprint.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning**, Preprint.![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki)

- **Continual Learning for Encoder-only Language Models via a Discrete Key-Value Bottleneck**, Preprint.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **CPRM: A LLM-based Continual Pre-training Framework for Relevance Modeling in Commercial Search**, Preprint.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training**, Preprint.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Federated Incremental Named Entity Recognition**, Preprint.![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen)

- **Reviving Dormant Memories: Investigating Catastrophic Forgetting in Language Models through Rationale-Guidance Difficulty**, Preprint.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Exploring Forgetting in Large Language Model Pre-Training**, Preprint.![](https://img.shields.io/badge/Mechanism-gray)

- **Efficient Continual Pre-training of LLMs for Low-resource Languages**, Preprint.![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange)

- **EvoWiki: Evaluating LLMs on Evolving Knowledge**, Preprint.![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-orange)

#### 2024 (2024.08-2024.10) üìÖ

- **WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**, NIPS 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning**, NIPS 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models**, NIPS 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) 

- **TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models**, NIPS 2024 Workshop. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow)

- **Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack**, NIPS 2024 (Database&Benchmark Track)![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) 

- **Does RoBERTa Perform Better than BERT in Continual Learn- ing: An Attention Sink Perspective**, CoLM.![](https://img.shields.io/badge/Mechanism-gray)

- **LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models**, EMNLP 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Lifelong Event Detection via Optimal Transport**, EMNLP 2024.![](https://img.shields.io/badge/Continual_Relation_Extraction-blue)

- **Should We Really Edit Language Models? On the Evaluation of Edited Language Models**, Preprint 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Exploring Continual Fine-Tuning for Enhancing Language Ability in Large Language Model**, Preprint 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **ToW: Thoughts of Words Improve Reasoning in Large Language Models**, Preprint 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **SeQuiFi: Mitigating Catastrophic Forgetting in Speech Emotion Recognition with Sequential Class-Finetuning**, Preprint 2024.(Speech)

- **In-context KV-Cache Eviction for LLMs via Attention-Gate**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **In Praise of Stubbornness: The Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Structure-aware Domain Knowledge Injection for Large Language Models**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance**, Preprint.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Amuro and Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **ToEdit: How to Synthesize Text Data to Avoid Model Collapse?**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Math for AI: On the Generalization of Learning Mathematical Problem Solving**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange)

- **Locate-then-Unlearn: An Effective Method of Multi-Task Continuous Learning for Large Language Models**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **GE-PEFT: Gated Expandable Parameter-Efficient Fine-Tuning for Continual Learning**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **Language Models as Feature Extractors for Accurate Continual Learning**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Text_Classification-green)

- **Towards Efficient and No Forgetting Domain Continual Pretraining by Mitigating the Stability Gap**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Data Efficient Continual Learning of Large Language Model**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data**, Preprint Openreview 2024.![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange)

- **Continual Memorization of Factoids in Large Language Models**, Preprint Openreview 2024.![](https://img.shields.io/badge/Mechanism-gray)

- **Contextual Experience Replay for Continual Learning of Language Agents**, Preprint Openreview 2024.![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige)

- **Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs**, Preprint 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Is Parameter Collision Hindering Continual Learning in LLMs?**, Preprint 2024.![](https://img.shields.io/badge/Mechanism-gray)

- **SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture**, Preprint 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **A Closer Look at Machine Unlearning for Large Language Models**, Preprint 2024.![](https://img.shields.io/badge/Mechanism-gray)



- **LeanAgent: Lifelong Learning for Formal Theorem Proving**, Preprint 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models**, Preprint 2024.![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange)

- **Learning Attentional Mixture of LoRAs for Language Model Continual Learning**, Preprint 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models**, Preprint 2024.![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige)

- **Synthetic continued pretraining**, Preprint 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **Alleviating Hallucinations in Large Language Models with Scepticism Modeling**, Preprint 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)

- **A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio**, Preprint 2024.![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red)![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange)

- **RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining**, Preprint 2024.![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-red)

- **PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**, Preprint 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Enhance Lifelong Model Editing with Continuous Data-Adapter Association**, Preprint 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin**, ACL 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning**, ACL 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models**, ACL 2024.![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Continual Dialogue State Tracking via Reason-of-Select Distillation**, Findings of ACL 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

- **Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System**, Findings of ACL 2024.![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple)

#### 2024 (2024.05-2024.07) üìÖ

- **Unlocking Continual Learning Abilities in Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2406.17245)] [[Code](https://github.com/wenyudu/MIGU)]

- **Large Language Model Can Continue Evolving From Mistakes**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2404.08707)]


- **COPAL: Continual Pruning in Large Language Generative Models**, ICML 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2405.02347)]

- **Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2404.07143)]

- **AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2404.08417)]

- **Efficient Continual Pre-training for Building Domain Specific Large Language Models**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2311.08545)]

- **Mitigating Catastrophic Forgetting in Language Transfer via Model Merging**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2407.08699v1)]

- **Reuse, Don‚Äôt Retrain: A Recipe for Continued Pretraining of Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://arxiv.org/pdf/2407.07263v1)]

- **Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://www.arxiv.org/pdf/2407.02118)]

- **MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://arxiv.org/pdf/2407.00875)]

- **Leitner-Guided Memory Replay for Cross-lingual Continual Learning**, NAACL 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://aclanthology.org/2024.naacl-long.432.pdf)] [[Code](https://github.com/meryemmhamdi1/x-continuous-learning/tree/main/humanlearn)]

- **BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://arxiv.org/pdf/2406.11418)] [[Code](https://github.com/babylm/babylm_data_preprocessing)]

- **Improving Language Models Trained with Translated Data via Continual Pre-Training and Dictionary Learning Analysis**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://arxiv.org/pdf/2405.14277)]

- **Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://arxiv.org/pdf/2405.05496)]

- **ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks**, ACL 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://arxiv.org/pdf/2405.14211)]

- **XMC-Agent : Dynamic Navigation over Scalable Hierarchical Index for Incremental Extreme Multi-label Classification**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://openreview.net/pdf?id=RQTgBAGNoi)]

- **Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models**, ACL 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://arxiv.org/pdf/2312.07887)] [[Code](https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning)]

- **Flexible Weight Tuning and Weight Fusion Strategies for Continual Named Entity Recognition**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen)

- **Distilling Causal Effect of Data in Continual Few-shot Relation Learning**, LREC-COLING 2024. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2024.lrec-main.451.pdf)] [[Code](https://github.com/ywh140/CECF)]

- **Improving Continual Few-shot Relation Extraction through Relational Knowledge Distillation and Prototype Augmentation**, LREC-COLING 2024. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2024.lrec-main.767.pdf)]

- **Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild**, ACL 2024. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://arxiv.org/pdf/2305.07085)]

- **An Ensemble-of-Experts Framework for Rehearsal-free Continual Relation Extraction**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue)

- **Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://openreview.net/pdf?id=vOqY1TqNYM)]

- **Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation**, ACL 2024. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://openreview.net/pdf?id=iT-OMlYozIs)]

- **Continual Dialogue State Tracking via Reason-of-Select Distillation**, ACL (Findings) 2024, ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2024.findings-acl.422.pdf)]

- **Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2406.12227v2)]

- **Learn it or Leave it: Module Composition and Pruning for Continual Learning**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2406.18708)] [[Code](https://github.com/boschresearch/MoCL-Pruning)]

- **Dirichlet Continual Learning: Tackling Catastrophic Forgetting in NLP**, UAI 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://openreview.net/pdf?id=jve2maFPzf)]

- **PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2406.12593)]

- **Revisiting Catastrophic Forgetting in Large Language Model Tuning**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2406.04836)] [[Code](https://github.com/Li-Hyn/LLM_CatastrophicForgetting)]

- **HFT: Half Fine-Tuning for Large Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2404.18466)]

- **Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System**, ACL 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2405.10992)]

- **SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models**, ACL 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2401.08295)] [[Code](https://github.com/circle-hit/SAPT)]

- **Self-Evolving GPT: A Lifelong Autonomous Experiential Learner**, ACL 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2407.08937)]

- **LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin**, ACL 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2312.09979)] [[Code](https://github.com/Ablustrund/LoRAMoE)]

- **Mitigate Negative Transfer with Similarity Heuristic Lifelong Prompt Tuning**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2406.12251)] [[Code](https://github.com/wcyno23/SHLPT)]

- **Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning**, NAACL(Findings) 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2404.14607)]

- **Sequential Editing for Lifelong Training of Speech Recognition Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://www.arxiv.org/pdf/2406.17935)]

- **Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://arxiv.org/pdf/2405.03279)]

- **Detoxifying Large Language Models via Knowledge Editing**, ACL 2024. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://arxiv.org/pdf/2403.14472)] [[Code](https://www.zjukg.org/project/SafeEdit/)]

- **Model Editing at Scale leads to Gradual and Catastrophic Forgetting**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://arxiv.org/pdf/2401.07453)]

- **Can We Continually Edit Language Models? On the Knowledge Attenuation in Sequential Model Editing**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black)

- **Incremental Sequential Labeling: A Tale of Two Shifts**, ACL 2024 (Findings). ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://arxiv.org/pdf/2402.10447)]

- **HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://arxiv.org/pdf/2405.14831)] [[Code](https://github.com/OSU-NLP-Group/HippoRAG)]

- **Towards Practical Tool Usage for Continually Learning LLMs**, Preprint 2024. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://arxiv.org/pdf/2404.09339)]

- **LoRA Learns Less and Forgets Less**, Preprint 2024. ![](https://img.shields.io/badge/Mechanism-gray) [[pdf](https://arxiv.org/pdf/2405.09673)]


#### 2024 (2024.01-2024.04) üìÖ
- **CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2402.16767)] [[Code](https://github.com/Sherlock-coder/CorpusBrainPlusPlus)]

- **Examining Forgetting in Continual Pre-training of Aligned Large Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2401.03129)] [[Code](https://github.com/lca0503/Llama_tw)]

- **Rho-1: Not All Tokens Are What You Need**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2404.07965)] [[Code](https://github.com/microsoft/rho)]

- **SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling**, NAACL 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2312.15166)]

- **LLaMA Pro: Progressive LLaMA with Block Expansion**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2401.02415)] [[Code](https://github.com/TencentARC/LLaMA-Pro)]

- **Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning**, AAAI 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/28466/28907)] [[Code](https://github.com/yangbang18/CLFM)]

- **Simple and Scalable Strategies to Continually Pre-train Large Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://arxiv.org/pdf/2403.08763)]

- **Set the Clock: Temporal Alignment of Pretrained Language Models**, ACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) [[pdf](https://arxiv.org/pdf/2402.16797)] [[Code](https://github.com/yizhongw/llm-temporal-alignment)]

- **HOP to the Next Tasks and Domains for Continual Learning in NLP**, AAAI 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/29349/30546
)]

- **Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://arxiv.org/pdf/2402.12220)] [[Code](https://recherchetts.github.io/bayesian-peft/)]

- **Rehearsal-Free Modular and Compositional Continual Learning for Language Models**, NAACL 2024. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://arxiv.org/pdf/2404.00790)]

- **Few-shot Incremental Event Detection**, TALLIP 2024. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://dl.acm.org/doi/pdf/10.1145/3634747)]

- **Self-generated Replay Memories for Continual Neural Machine Translation**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://arxiv.org/pdf/2403.13130)] [[Code](https://github.com/m-resta/sg-rep)]

- **F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation**, NAACL 2024. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://arxiv.org/pdf/2404.04846)] [[Code](https://github.com/WJMacro/ContinualMT)]

- **SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2401.08295)]

- **InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions**, NAACL 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2403.11435)]

- **Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2402.18865)] [[Code](https://github.com/which47/LLMCL)]

- **Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal**, ACL 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2403.01244)]

- **Scalable Language Model with Generalized Continual Learning**, ICLR 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://openreview.net/pdf?id=mz8owj4DXu)] [[Code](https://github.com/Pbihao/SLM)]

- **Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language**, NAACL (Findings) 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2404.14607)]

- **MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://arxiv.org/pdf/2402.11260)]

- **WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://arxiv.org/pdf/2402.10987)]

- **CPPO: Continual Learning for Reinforcement Learning with Human Feedback**, ICLR 2024. ![](https://img.shields.io/badge/Continual_Alignment-lightgray) [[pdf](https://openreview.net/pdf?id=86zAUE80pP)] [[Code](https://openi.pcl.ac.cn/Hanlard/CPPO)]

- **COPR: Continual Human Preference Learning via Optimal Policy Regularization**, Preprint 2024. ![](https://img.shields.io/badge/Continual_Alignment-lightgray) [[pdf](https://arxiv.org/pdf/2402.14228)] [[Code](https://openi.pcl.ac.cn/Hanlard/Offline_alignment_methods_based_on_trlx.git)]

- **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**, ICLR 2024. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://openreview.net/pdf?id=hSyW5go0v8)] [[Code](https://github.com/AkariAsai/self-rag)]

- **LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error**, Preprint 2024. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://arxiv.org/pdf/2403.04746)]

- **Towards Practical Tool Usage for Continually Learning LLMs**, Preprint 2024. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://arxiv.org/pdf/2404.09339)]

- **Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum**, AAAI 2024. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/29759/31307)]

- **Toolllm: Facilitating large language models to master 16000+ real-world apis**, ICLR 2024. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://openreview.net/pdf?id=dHng2O0Jjr)] [[Code](https://github.com/OpenBMB/ToolBench)]


#### 2023 üìÖ
- **Continual Pre-Training of Large Language Models: How to (re)warm your model?**, ICML (Workshop) 2023. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://openreview.net/pdf?id=pg7PUJe0Tl)]

- **EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data**, Preprint 2023. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2312.15696)]

- **QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search**, KDD 2023. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://dl.acm.org/doi/pdf/10.1145/3580305.3599891)] [[Code](https://github.com/hsaest/QUERT)]

- **Recyclable Tuning for Continual Pre-training**, ACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://aclanthology.org/2023.findings-acl.723.pdf)] [[Code](https://github.com/thunlp/RecyclableTuning)]

- **Large Language Models Encode Clinical Knowledge**, Nature 2023. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2212.13138)]

- **Continual Learning Under Language Shift**, Preprint 2023. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://arxiv.org/pdf/2311.01200v3)]

- **Exploring Continual Learning for Code Generation Models**, ACL 2023. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://aclanthology.org/2023.acl-short.68.pdf)] [[Code](https://github.com/amazon-science/codetask-cl-pptf)]

- **Lifelong Language Pretraining with Distribution-Specialized Experts**, ICML 2023. ![](https://img.shields.io/badge/Continual_Language_Domain_Pretraining-orange) [[pdf](https://openreview.net/pdf?id=Q4QFG5Fe4O)]

- **Mitigating Catastrophic Forgetting in Task-Incremental Continual Learning with Adaptive Classification Criterion**, Preprint 2023. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://arxiv.org/pdf/2305.12270)]

- **Rehearsal-free Continual Language Learning via Efficient Parameter Isolation**, ACL 2023. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2023.acl-long.612.pdf)] [[Code](https://github.com/Dicer-Zz/EPI)]

- **Class-Incremental Learning based on Label Generation**, ACL 2023. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2023.acl-short.109.pdf)] [[Code](https://github.com/shaoyijia/VAG)]

- **Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning**, ACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2023.findings-acl.48.pdf)]

- **InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective**, EMNLP 2023. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2023.findings-emnlp.969.pdf)] [[Code](https://github.com/Yifan-Song793/InfoCL)]

- **ConPET: Continual Parameter-Efficient Tuning for Large Language Models**, Preprint 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://arxiv.org/pdf/2309.14763)] [[Code](https://github.com/Raincleared-Song/ConPET)]

- **A Neural Span-Based Continual Named Entity Recognition Model**, AAAI 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/26638/26410)] [[Code](https://github.com/Qznan/SpanK)]

- **Learning ‚ÄúO‚Äù Helps for Learning More: Handling the Unlabeled Entity Problem for Class-incremental NER**, ACL 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2023.acl-long.328.pdf)] [[Code](https://github.com/rtmaww/O_CILNER)]

- **Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction**, ACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.findings-acl.141.pdf)] [[Code](https://github.com/VT-NLP/ICE)]

- **ProtoNER: Few Shot Incremental Learning for Named Entity Recognition Using Prototypical Networks**, BPM 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://arxiv.org/pdf/2310.02372)]

- **Task Relation Distillation and Prototypical Pseudo Label for Incremental Named Entity Recognition**, CIKM 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://arxiv.org/pdf/2308.08793)] [[Code](https://github.com/BladeDancer957/INER_RDP)]

- **Continual Named Entity Recognition without Catastrophic Forgetting**, CIKM 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2023.emnlp-main.509.pdf)] [[Code](https://github.com/BladeDancer957/CPFD)]

- **SKD-NER:Continual Named Entity Recognition via Span-based Knowledge Distillation with Reinforcement Learning**, EMNLP 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2023.emnlp-main.413.pdf)]

- **Novel Slot Detection With an Incremental Setting**, EMNLP (Findings) 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2023.findings-emnlp.53.pdf)] [[Code](https://github.com/cs-liangchen-work/NovelIE)]

- **Incremental event detection via an improved knowledge distillation based model**, Neurocomputing 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231223X00289/1-s2.0-S0925231223006422/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIEK1bj6Ly8ruhs9a2Zb2c5uJ49vKzMhJlcbiIbJJpcIZAiAFc1xzJtgK%2FpO%2Fkbz2KJxccOqTyVgGBW7HZ%2BEDgnGcVyq8BQjv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMWcVdnvyvyvCOf613KpAFTU%2FIhOI61FrBYfhW7wvTWc9Iia5pvrE5PmD2tHyfFtCTvkgVYnDSXfnIsGK%2FZMF0jZARLZ5NlXyPfR1%2B7HZiZ1vaDvadAfG0sLKtY%2ByplTKXA0FzrfRBcj0oG6%2F90Kf7bKQbj6eN3JexNRUIHneHOWcrlFHbZYuIroDvXNKa9ieEBj0pDBMFckBbvc%2BwuNzsZbipC5581zjP4joiiPf1oxw1VaClYKxGzRsyZGIwN5UxNX3kWxuAccBS%2FSDRcA2XpIg8wPEVT16FFSYmrK2tZ2tk2OjKGIu66Md9nUS4YV3BK5SCVGEfTug18d5Etwn%2FpTouhyVBcjUDF0ZR6bKREbFJyQQa6fC2W%2B62JmyYLxu2QSmqI%2F8SBV0a5C7KaYxiqHl0Ot4Yy%2BsIRe8zFkyw1EzLC%2BZEMED5KLyDztFn15mYnLdUu9sIrUBf6cXvmSzhDi7%2FSou7Teww4YFQwdQ6bnfyRZgeF%2B5BKli9d9WUWZmQE5Hp%2FAcQxGtdFCMiOkiB31uIbcbaBIu4%2Fg1WzNj0dtyu%2Fdrxahce2QdqyXpV5fLMepPzvNR1n6TzAX6K0otDDa3tX%2Fd%2Bzwe1TRX7DpkDDuC5yFWnZOAT7lGssLMH9aHZf0SZSv1JPxHk41ZY9MXiXXFqawtL1%2BcIoxoKVN3wJF6BGfcp3Yz98jv13D316X841Haitq85oBuwXCdb9QH6nKDZFn2NhinCVC5UhiRGTJItPxKdO9RKAsdrjX3lhJHiMT%2FOhUWAf%2BYvPdV1ukxUfgiA0ZHcqQ8AL4Rg1%2B9GPZZvTjgr6HbVRzsGUgNbRE38XJy0RAfuBPfOQcyyC2q6Y%2BdXj9Qxhcj7cErgw2Y15s4Gbu9O3RT3YDCmIOGCU4YwweeisgY6sgGyC5MQGJB5takEqm5cLtpQC2HodtOM70AANOdIQSGE%2Fl1d9JswbIXx2nyGlFFmxm6426QLCkdHe5PHWAhpLkmKWyN6X8krQ3H5617W%2FUe1cTM0SLHkKi6SoDe0QfpBiTNpKPQn%2BuczGYP3YeFGybQEaet5nWtK0YOd41MCQO2wWVHK1CuLQwJ9ZYlef4Aq9wtDXVpCSRaGfc1TmmolhHDOfC7YZY7dQNtHVz8GPYGHLjBp&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240518T150903Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&sX-Amz-Credential=ASIAQ3PHCVTYRJJBNUDJ%2F20240518%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=958666ecc95b23232cf77236b4e51d1057271ab605b99aaf87ac92cfc0f904a7&hash=9b3545292353e6576b86cbb5b53f1cf0563d2f2676821cdfd397782aa16669d9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0925231223006422&tid=spdf-130a1314-9dc9-458d-8716-9d499548602c&sid=3682f588522cd0438f8ac403cedfc1eb08f0gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0d095d565b0d0355&rr=885cc41e4dff84b5&cc=cn)]

- **Decomposing Logits Distillation for Incremental Named Entity Recognition**, SIGIR 2023. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://dl.acm.org/doi/pdf/10.1145/3539618.3591970)]

- **Consistent Prototype Learning for Few-Shot Continual Relation Extraction**, ACL 2023. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.acl-long.409.pdf)] [[Code](https://github.com/XiudiChen/ConPL)]

- **Enhancing Continual Relation Extraction via Classifier Decomposition**, ACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.findings-acl.638.pdf)] [[Code](https://github.com/hemingkx/CDec)]

- **Improving Continual Relation Extraction by Distinguishing Analogous Semantics**, ACL 2023. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.acl-long.65.pdf)] [[Code](https://github.com/nju-websoft/CEAR)]

- **Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction**, ACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.findings-acl.804.pdf)] [[Code](https://github.com/nju-websoft/SCKD)]

- **ICA-Proto: Iterative Cross Alignment Prototypical Network for Incremental Few-Shot Relation Classification**, EACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.findings-eacl.171.pdf)]

- **Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction**, ACL (Findings) 2023. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2023.findings-acl.804.pdf)] [[Code](https://github.com/nju-websoft/SCKD)]

- **Continual Knowledge Distillation for Neural Machine Translation**, ACL 2023. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2023.acl-long.443.pdf)] [[Code](https://github.com/THUNLP-MT/CKD)]

- **Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation**, ACL 2023. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2023.acl-long.852.pdf)] [[Code](https://github.com/THUNLP-MT/ktnmt)]

- **Continual Learning for Multilingual Neural Machine Translation via Dual Importance-based Model Division**, EMNLP 2023. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2023.emnlp-main.736.pdf)] [[Code](https://github.com/raburabu91/BVP4CL)]

- **Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning**, ACL 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2023.acl-long.16.pdf)] [[Code](https://github.com/jokieleung/Lottery_Prompt)]

- **Large-scale Lifelong Learning of In-context Instructions and How to Tackle It**, ACL 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2023.acl-long.703.pdf)]

- **Generative Replay Inspired by Hippocampal Memory Indexing for Continual Language Learning**, EACL 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2023.eacl-main.65.pdf)] [[Code](https://github.com/arumaekawa/GR-HMI)]

- **Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation**, EMNLP 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2023.emnlp-main.414.pdf)]

- **Orthogonal Subspace Learning for Language Model Continual Learning**, EMNLP (Findings) 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2023.findings-emnlp.715.pdf)] [[Code](https://github.com/cmnfriend/O-LoRA)]

- **Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks**, EMNLP (Findings) 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2023.findings-emnlp.1008.pdf)]

- **Progressive Prompts: Continual Learning for Language Models**, ICLR 2023. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://openreview.net/pdf?id=UJTgQBc91_)] [[Code](https://github.com/arazd/ProgressivePrompts)]

- **Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models**, EACL 2023. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://aclanthology.org/2023.eacl-main.199.pdf)] [[Code](https://github.com/peterbhase/SLAG-Belief-Updating)]

- **Aging with GRACE: Lifelong Model Editing with Key-Value Adaptors**, ICLR 2023. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://openreview.net/pdf?id=ngCT1EelZk)]

- **Transformer-Patcher: One Mistake worth One Neuron**, ICLR 2023. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://openreview.net/pdf?id=4oYUGeGBPm)] [[Code](https://github.com/ZeroYuHuang/Transformer-Patcher)]

- **Mitigating the Alignment Tax of RLHF**, Preprint 2023. ![](https://img.shields.io/badge/Continual_Alignment-lightgray) [[pdf](https://arxiv.org/pdf/2309.06256)]

- **Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models**, EMNLP 2023. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://aclanthology.org/2023.emnlp-main.63.pdf)] [[Code](https://github.com/gankim/tree-of-clarifications)]

- **Active Retrieval Augmented Generation**, EMNLP 2023. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://aclanthology.org/2023.emnlp-main.495.pdf)] [[Code](https://github.com/jzbjyb/FLARE)]

- **Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions**, ACL 2023. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://aclanthology.org/2023.acl-long.557.pdf)] [[Code](https://github.com/stonybrooknlp/ircot)]

- **Toolalpaca: Generalized tool learning for language models with 3000 simulated cases**, Preprint 2023. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://arxiv.org/pdf/2306.05301)] [[Code](https://github.com/tangqiaoyu/ToolAlpaca)]

- **Gorilla: Large language model connected with massive apis**, Preprint 2023. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://arxiv.org/pdf/2305.15334)] [[Code](https://github.com/ShishirPatil/gorilla)]

- **Chameleon: Plug-and-play compositional reasoning with large language models**, NIPS 2023. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://openreview.net/pdf?id=HtqnVSCj3q)] [[Code](https://github.com/lupantech/chameleon-llm)]

- **Toolformer: Language Models Can Teach Themselves to Use Tools**, NIPS 2023. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://openreview.net/pdf?id=Yacmpz84TH)]

- **GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction**, NIPS 2023. ![](https://img.shields.io/badge/Tool_Based_Lifelong_Learning-khaki) [[pdf](https://openreview.net/pdf?id=cwjh8lqmOL)] [[Code](https://github.com/AILab-CVC/GPT4Tools)]


#### 2022 üìÖ
- **Continual Pre-Training Mitigates Forgetting in Language and Vision**, Preprint 2022. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://arxiv.org/pdf/2205.09357)] [[Code](https://github.com/AndreaCossu/continual-pretraining-nlp-vision)]

- **ELLE: Efficient Lifelong Pre-training for Emerging Data**, ACL (Findings) 2022. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://aclanthology.org/2022.findings-acl.220.pdf)] [[Code](https://github.com/thunlp/ELLE)]

- **Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora**, ACL (Workshop) 2022. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://aclanthology.org/2022.bigscience-1.1.pdf)]

- **bert2BERT: Towards Reusable Pretrained Language Models**, ACL 2022. ![](https://img.shields.io/badge/Continual_Vertical_Domain_Pretraining-red) [[pdf](https://aclanthology.org/2022.acl-long.151.pdf)] [[Code](https://github.com/huawei-noah/Pretrained-Language-Model)]

- **Time Waits for No One! Analysis and Challenges of Temporal Misalignment**, NAACL 2022. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) [[pdf](https://aclanthology.org/2022.naacl-main.435.pdf)] [[Code](https://github.com/Kel-Lu/time-waits-for-no-one)]

- **TimeLMs: Diachronic Language Models from Twitter**, ACL 2022. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) [[pdf](https://aclanthology.org/2022.acl-demo.25.pdf)] [[Code](https://github.com/cardiffnlp/timelms)]

- **Continual Few-shot Intent Detection**, COLING 2022. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2022.coling-1.26.pdf)]

- **Incremental Intent Detection for Medical Domain with Contrast Replay Networks**, ACL (Findings) 2022. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2022.findings-acl.280.pdf)]

- **Continual training of language models for few-shot learning**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2022.emnlp-main.695.pdf)]

- **Parameter-efficient Continual Learning Framework in Industrial Real-time Text Classification System**, NAACL 2022. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2022.naacl-industry.35.pdf)]

- **Prompt Augmented Generative Replay via Supervised Contrastive Learning for Lifelong Intent Detection**, NAACL (Findings) 2022. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2022.findings-naacl.84.pdf)]

- **Memory Efficient Continual Learning with Transformers**, NIPS 2022. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://openreview.net/pdf?id=U07d1Y-x2E)]

- **Few-Shot Class-Incremental Learning for Named Entity Recognition**, ACL 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2022.acl-long.43.pdf)]

- **Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples**, ACL (Findings) 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2022.findings-acl.179.pdf)]

- **Incremental Prompting: Episodic Memory Prompt for Lifelong Event Dectection**, COLING 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2022.coling-1.189.pdf)] [[Code](https://github.com/VT-NLP/Incremental_Prompting)]

- **Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2022.emnlp-main.236.pdf)] [[Code](https://github.com/zzz47zzz/CFNER)]

- **BNU: A Balance-Normalization-Uncertainty Model for Incremental Event Detection**, ICASSP 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747708)]

- **Similarity-Driven Adaptive Prototypical Network for Class-incremental Few-shot Named Entity Recognition**, ICTAI 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10097986)]

- **HEFT: A History-Enhanced Feature Transfer framework for incremental event detection**, KBS 2022. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705122X00166/1-s2.0-S0950705122008061/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIDGej%2BZ9m2J2ytK5w36KWKcEv3gThuaWIHufatpHidkTAiEAjJYEcHYhW01TAkVvFeUvqUbfrBdHwQcYMnxxmjet93YquwUI7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDKzZZZZI3SMAHjMbuyqPBaAhyjLItjCl3zFmft%2BxDfWV2MfRrhy5%2Bdf5DpBoJjDrC4IrsKr5iocT0pD31pIiLMMSElG7KE84hTmE8dgkX5%2FRloLcvTuKCjuVs6fVJvRCspsveqr7upGzXl0DgKrMbCsGO%2BegT%2BDP7YcoHveQbZIryNAucTz7YJAU5NzAaF9ZbMV8UkiGfiVeo4PzK7hfliJuIYhpmCOE9PQsmuXlOlJIojXndgiJwW0x53Zk6zji9oxbbtHkCtDtG4fmK%2BAcjjarclM5Ih9iSHSU32N1Ez8FTGLt98Ta6T4AdIUjZynMR%2FVQCg4s9w%2BZup%2FUma3vsKvCdUGTNLAhugf6k9Y2p5%2FkEpyzxAuVAE5cbkthVbfZwAbpQZZlausIsq8zZoj%2F9A9liO8h4L4s%2BJ5WVA5sH3WTMAlVwFoW6nNIGSj3tbhWWudVvkrBcqTW5DnG%2BTSKH4Qns%2FEweyusiLmzx0eTQB9JjVdwgDs5iFO2KvQdmn%2F2NfLjSKJY1mO0TN%2BLn1f5G%2FyCBmthNJyNz0dyrUzkt5u%2Fvp6SBXXbwezklK%2FZJiAgKK7m4P4ocS%2B5yC%2B1ueXS2REcznQRFRzTIhU6kcT4uDt92wqIIaN23jQ16QVgn8U27%2Fn1kzEFqqh5FFly3r6zYOFgNtz8OuX9vdCif3pIwuZb9TDpCdQ6mIC7z3pWHzFwvHPBhmzHmI7WMSn%2FROlEXid2%2FpzFSe4laBDIgQv0yDa8BxlKHlZxt3vHi51z%2FXgZRBttoRsnU4H2cBPhUBoxDvcgsEntJF%2Br4Lm2DTFOm2mwKiND7tBcijwxErP9caOOZ6ZdLSeU9NYXYdrC8yMhUIFEtG9ysZaFLBp6AjQeCNIJsXrXrbBmmh3t8xukPLUwrumisgY6sQFh7rJYe3CCejqSjvILn5KdJ%2BE%2BiPN1D7zZ%2Fun1nTwKoH3U%2FktdVr%2FOmjv22mOdUduSvEv69pq97JwEYOmlIp42GlobQ9kzOriOiJ727M92UfjMhSPrwTcSmvLeTb54gLhmcLuiS0WZBdUfx%2F2XEYuahDYuLrcbCTH2m4InSytNbOeAWWIlTAnosDjEsHd2JlSk1V70ej13StBo5ZXYjfNllXbd2R%2Fw1kMOoYZTAK9QI9E%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240518T150930Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY5532AY56%2F20240518%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0f8e27f71a9a02306eee68911d2f0c2052ddbb046c6b91e476011c97eb80ec65&hash=e2913561864f523ed8763a508a574ea73fa2022b9b6490a137d91cdc987b830a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0950705122008061&tid=spdf-4425fead-8428-44a4-85ca-dc5f5464a640&sid=3682f588522cd0438f8ac403cedfc1eb08f0gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0d095d565b0d0355&rr=885cc4c5cb9284b5&cc=cn)]

- **Consistent Representation Learning for Continual Relation Extraction**, ACL (Findings) 2022. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2022.findings-acl.268.pdf)] [[Code](https://github.com/thuiar/CRL)]

- **Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation**, ACL 2022. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2022.acl-long.198.pdf)] [[Code](https://github.com/qcwthu/Continual_Fewshot_Relation_Learning)]

- **Less is More: Rethinking State-of-the-art Continual Relation Extraction Models with a Frustratingly Easy but Effective Approach**, Preprint 2022. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2022.acl-long.198.pdf)]

- **Improving continual relation extraction through prototypical contrastive learning**, COLING 2022. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2022.coling-1.163.pdf)] [[Code](https://github.com/PaperDiscovery/CRECL)]

- **Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2022.emnlp-main.420.pdf)] [[Code](https://github.com/Wangpeiyi9979/ACA)]

- **Prompt-based prototypical framework for continual relation extraction**, TASLP 2022. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://dl.acm.org/doi/pdf/10.1109/TASLP.2022.3199655)]

- **Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation**, ACL 2022. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2022.acl-long.143.pdf)] [[Code](https://github.com/ictnlp/COKD)]

- **CLLE: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation**, EMNLP (Findings) 2022. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2022.findings-emnlp.30.pdf)] [[Code](https://github.com/HITSZ-HLT/CLLE)]

- **Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2022.emnlp-main.111.pdf)] [[Code](https://github.com/ictnlp/LFR-NMT)]

- **Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2022.emnlp-main.720/)] [[Code](https://github.com/koukaiu/evs)]

- **Continual Sequence Generation with Adaptive Compositional Modules**, ACL 2022. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2022.acl-long.255.pdf)] [[Code](https://github.com/GT-SALT/Adaptive-Compositional-Modules)]

- **ConTinTin: Continual Learning from Task Instructions**, ACL 2022. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2022.acl-long.218.pdf)]

- **Continual Prompt Tuning for Dialog State Tracking**, ACL 2022. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2022.acl-long.80.pdf)] [[Code](https://github.com/thu-coai/cpt4dst)]

- **Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2022.emnlp-main.766.pdf)] [[Code](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/pcll)]

- **Fine-tuned Language Models are Continual Learners**, EMNLP 2022. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2022.emnlp-main.410.pdf)] [[Code](https://github.com/ThomasScialom/T0_continual_learning)]

- **LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5**, ICLR 2022. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://openreview.net/pdf?id=HCRVf71PMF)] [[Code](https://github.com/qcwthu/Lifelong-Fewshot-Language-Learning)]

- **Plug-and-Play Adaptation for Continuously-updated QA**, ACL (Findings) 2022. ![](https://img.shields.io/badge/Continual_Knowledge_Editing-black) [[pdf](https://aclanthology.org/2022.findings-acl.37.pdf)] [[Code](https://github.com/wookjeHan/Continual-Plug-and-Adapt-for-CuQA/)]


#### 2021 üìÖ
- **Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media**, EMNLP (Findings) 2021. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) [[pdf](https://aclanthology.org/2021.findings-emnlp.206.pdf)] [[Code](https://github.com/paul-rottger/temporal-adaptation)]

- **ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning**, EMNLP 2021. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) [[pdf](https://aclanthology.org/2021.emnlp-main.436.pdf)] [[Code](https://github.com/PlusLabNLP/ECONET)]

- **Mind the Gap: Assessing Temporal Generalization in Neural Language Models**, NIPS 2021. ![](https://img.shields.io/badge/Continual_Temporal_Domain_Pretraining-yellow) [[pdf](https://openreview.net/pdf?id=73OmmrCfSyy)] [[Code](https://github.com/google-deepmind/deepmind-research/tree/master/pitfalls_static_language_models)]

- **Learning to Solve NLP Tasks in an Incremental Number of Languages**, ACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.acl-short.106.pdf)]

- **Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.naacl-main.378.pdf)] [[Code](https://github.com/ZixuanKe/PyContinual)]

- **CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks**, EMNLP 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.emnlp-main.550.pdf)] [[Code](https://github.com/ZixuanKe/PyContinual)]

- **Continual Learning for Text Classification with Information Disentanglement Based Regularization**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.naacl-main.218.pdf)] [[Code](https://github.com/GT-SALT/IDBR)]

- **Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.naacl-main.212.pdf)] [[Code](https://github.com/tinghua-code/CCFI)]

- **Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.naacl-main.106.pdf)] [[Code](https://github.com/congyingxia/IncrementalFSTC)]

- **Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning**, NIPS 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/bcd0049c35799cdf57d06eaf2eb3cff6-Paper.pdf)] [[Code](https://github.com/ZixuanKe/PyContinual)]

- **Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification**, SIGIR 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://dl.acm.org/doi/pdf/10.1145/3404835.3462902)] [[Code](https://github.com/siat-nlp/IPRLS)]

- **Lifelong Intent Detection via Multi-Strategy Rebalancing**, Preprint 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://arxiv.org/pdf/2106.11197)]

- **Lifelong Knowledge-Enriched Social Event Representation Learning**, EACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.eacl-main.317.pdf)] [[Code](https://pralav.github.io/lifelong_eventrep/?c=10)]

- **Lifelong Learning of Hate Speech Classification on Social Media**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Text_Classification-green) [[pdf](https://aclanthology.org/2021.naacl-main.183.pdf)]

- **Continual Learning for Named Entity Recognition**, AAAI 2021. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17600/17407)]

- **Lifelong Event Detection with Knowledge Transfer**, EMNLP 2021. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2021.emnlp-main.428.pdf)] [[Code](https://github.com/Perfec-Yu/Lifelong-ED)]

- **Curriculum-meta learning for order-robust continual relation extraction**, AAAI 2021. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17241/17048)] [[Code](https://github.com/wutong8023/AAAI-CML)]

- **Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction**, ACL 2021. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2021.acl-long.20.pdf)] [[Code](https://github.com/fd2014cl/RP-CRE)]

- **Continual learning in multilingual nmt via language-specific embeddings**, WMT 2021. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2021.wmt-1.62/)]

- **Continual Learning for Neural Machine Translation**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2021.naacl-main.310/)] [[Code](https://github.com/caoy1996/CLNMT)]

- **Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution**, NAACL 2021. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2021.naacl-main.93.pdf)]

- **Rational LAMOL: A rationale-based lifelong learning framework**, ACL 2021. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2021.acl-long.229.pdf)] [[Code](https://github.com/kanwatchara-k/r_lamol)]

- **Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking**, ACL 2021. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2021.acl-short.66.pdf)] [[Code](https://github.com/siat-nlp/TPEM)]

- **Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning**, EMNLP (Findings) 2021. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2021.findings-emnlp.62.pdf)] [[Code](https://github.com/INK-USC/CLIF)]

- **Continual Learning in Task-Oriented Dialogue Systems**, EMNLP 2021. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2021.emnlp-main.590.pdf)] [[Code](https://github.com/andreamad8/ToDCL)]


#### 2020 üìÖ
- **Incremental Event Detection via Knowledge Consolidation Networks**, EMNLP 2020. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/2020.emnlp-main.52.pdf)] [[Code](https://github.com/CPF-NLPR/IncrementalED)]

- **Continual Relation Learning via Episodic Memory Activation and Reconsolidation**, ACL 2020. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/2020.acl-main.573.pdf)] [[Code](https://github.com/thunlp/ContinualRE)]

- **Findings of the First Shared Task on Lifelong Learning Machine Translation**, WMT 2020. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/2020.wmt-1.2.pdf)]

- **Distill and Replay for Continual Language Learning**, COLING 2020. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2020.coling-main.318.pdf)]

- **Continual Learning for Natural Language Generation in Task-oriented Dialog Systems**, EMNLP (Findings) 2020. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/2020.findings-emnlp.310.pdf)] [[Code](https://github.com/MiFei/Continual-Learning-for-NLG)]

- **LAMOL: LAnguage MOdeling for Lifelong Language Learning**, ICLR 2020. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://openreview.net/pdf?id=Skgxcn4YDS)] [[Code](https://github.com/jojotenya/LAMOL)]

- **Dense Passage Retrieval for Open-Domain Question Answering**, EMNLP 2020. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://aclanthology.org/2020.emnlp-main.550.pdf)] [[Code](https://github.com/facebookresearch/DPR)]

- **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**, NIPS 2020. ![](https://img.shields.io/badge/Retrieval_Based_Lifelong_Learning-beige) [[pdf](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)] [[Code](https://github.com/huggingface/transformers)]


#### 2019 üìÖ
- **A Progressive Model to Enable Continual Learning for Semantic Slot Filling**, EMNLP 2019. ![](https://img.shields.io/badge/Continual_Named_Entity_Recognition-brightgreen) [[pdf](https://aclanthology.org/D19-1126.pdf)]

- **Meta-Learning Improves Lifelong Relation Extraction**, RepL4NLP 2019. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/W19-4326.pdf)]

- **Sentence Embedding Alignment for Lifelong Relation Extraction**, NAACL 2019. ![](https://img.shields.io/badge/Continual_Relation_Extraction-blue) [[pdf](https://aclanthology.org/N19-1086.pdf)] [[Code](https://github.com/hongwang600/Lifelong_Relation_Detection)]

- **From Bilingual to Multilingual Neural Machine Translation by Incremental Training**, JASIST 2019. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/P19-2033.pdf)]

- **Incremental Learning from Scratch for Task-Oriented Dialogue Systems**, ACL 2019. ![](https://img.shields.io/badge/Continual_Instruction_Tuning-purple) [[pdf](https://aclanthology.org/P19-1361.pdf)] [[Code](https://github.com/Leechikara/Incremental-Dialogue-System)]


#### 2018 üìÖ
- **Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation**, NGT 2018. ![](https://img.shields.io/badge/Continual_Machine_Translation-violet) [[pdf](https://aclanthology.org/W18-2705.pdf)] [[Code](https://github.com/khayrallah/OpenNMT-py-reg)]
